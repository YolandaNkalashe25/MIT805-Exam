# -*- coding: utf-8 -*-
"""Capstone Project Modelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FeEcj6omekwL0KDswL-rYZ5VO2d_pOzJ

### Importing libraries and Data
"""
import streamlit as st
import pandas as pd

# Commented out IPython magic to ensure Python compatibility.
# =============================================================================
# import pandas as pd # to read csv/excel formatted data
# import matplotlib
# import matplotlib.pyplot as plt # to plot graphs
# import numpy as np
# import seaborn as sns
# import missingno
# # %matplotlib inline
# from langdetect import detect
# from deep_translator import GoogleTranslator
# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
# import string
# from sklearn.cluster import KMeans
# from sklearn.cluster import MiniBatchKMeans
# from sklearn.decomposition import PCA
# from sklearn.manifold import TSNE
# import nltk
# import re
# from sklearn.linear_model import LogisticRegression
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import GridSearchCV
# from sklearn.pipeline import Pipeline
# from sklearn.svm import LinearSVC
# from sklearn.naive_bayes import MultinomialNB
# import sklearn.metrics as metrics
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
# from mlxtend.plotting import plot_confusion_matrix
# from datetime import datetime, timedelta, timezone
# # %matplotlib inline
# =============================================================================



# =============================================================================
# if uploaded_file is not None:
#     st.success("File uploaded successfully")
#     try:
#         data=pd.read_excel(uploaded_file())
# =============================================================================
# =============================================================================
def load_new_data(df):
         df['statuses_created_at'] =  pd.to_datetime(df['statuses_created_at'])
         df['statuses_created_at'] =  pd.to_datetime(df['statuses_created_at'],format="%Y-%m-%d")
         df['statuses_retweeted_status_created_at'] =  pd.to_datetime(df['statuses_retweeted_status_created_at'])
         df['statuses_retweeted_status_created_at'] =  pd.to_datetime(df['statuses_retweeted_status_created_at'],format="%Y-%m-%d") 
         df['statuses_retweeted_status_user_created_at'] =  pd.to_datetime(df['statuses_retweeted_status_user_created_at'])
         df['statuses_retweeted_status_user_created_at'] =  pd.to_datetime(df['statuses_retweeted_status_user_created_at'],format="%Y-%m-%d")
        
         return df
#=============================================================================

import datetime
 
def avg_day(date_col):
    time_series = pd.DataFrame(date_col.value_counts().reset_index())
    time_series.columns = ['date', 'count']
    time_series['tweetdate'] = time_series['date'].map(lambda x: x.date())
    grp_date = time_series.groupby('tweetdate')
    tweet_by_date = pd.DataFrame(grp_date.size(), columns=['num_'])
    tweet_by_date_avg=tweet_by_date.num_.mean()

    return tweet_by_date_avg  

def sub_df(data):
    hours_List=[]
    min_List=[]
    text_List=[]
    location_List=[]
    followers_List=[]
    retweetcount_List=[]
    listedcount_List=[]
    source_List=[]
    Length_List=[]

    input_query=[]
    iso_code=[]
    userdescription=[]
    user_Created=[]
    Days_Active=[]
    fav_Count=[]
    verif=[]
    user_Statuses=[]
    has_Image=[]
    ave_tweets_perday=[]
    id=[]
    
    from collections import Counter

    MyList_tweet = data['statuses_retweeted_status_id']
    k=Counter(MyList_tweet)

    D_Tweet_id=pd.DataFrame.from_dict(k, orient='index').reset_index()
 
    D_Tweet_id.columns=["statuses_retweeted_status_id","value"]
         
    stts_Id_top=(D_Tweet_id[D_Tweet_id["statuses_retweeted_status_id"].notna()])
         
    stts_Id_top=D_Tweet_id["statuses_retweeted_status_id"].to_list()
         
    stts_Id_top=[x for x in stts_Id_top if str(x) != 'nan']

    for i in stts_Id_top:
        X=data.loc[data['statuses_retweeted_status_id']==i]
        original_date= X.statuses_retweeted_status_created_at.min()
        max_date=X.statuses_created_at.max()
        diff=(X.statuses_created_at.max() - X.statuses_retweeted_status_created_at.min())
        days=diff.days
        seconds=diff.seconds
        hours=(days * 24) + (seconds // 3600)
        minutes = (seconds % 3600) // 60
        hours_List.append(hours)
        min_List.append(minutes)
  
        id_=i
        id.append(id_)
 
        text= X.statuses_retweeted_status_text.iloc[0]
        text_List.append(text)
 
        date_col=X['statuses_created_at']
        ave_day=avg_day(date_col)
        ave_tweets_perday.append(ave_day)
     
        Length=len(str(text))
        Length_List.append(Length)
     
        hashtag=X.input_query.iloc[0]
        input_query.append(hashtag)
    
        iso=X.statuses_metadata_iso_language_code.iloc[0]
        iso_code.append(iso)
   
        user_description=X.statuses_retweeted_status_user_description.iloc[0]
        userdescription.append(user_description)

        userCreated=X.statuses_retweeted_status_user_created_at.iloc[0]
        user_Created.append(userCreated)

        diff_Created=(X.statuses_retweeted_status_created_at.max() - X.statuses_retweeted_status_user_created_at.min())
        DaysActive=diff_Created.days
        Days_Active.append(DaysActive)

        favCount=X.statuses_retweeted_status_user_favourites_count.iloc[0]
        fav_Count.append(favCount)
 
        userStatuses=X.statuses_retweeted_status_user_statuses_count.iloc[0]
        user_Statuses.append(userStatuses)

        verified=X.statuses_retweeted_status_user_verified.iloc[0]
        verif.append(verified)
  
        hasImage=X.statuses_retweeted_status_user_profile_use_background_image.iloc[0]
        has_Image.append(hasImage)
 
        location=X.statuses_retweeted_status_user_location.iloc[0]
        location_List.append(location)
 
        user_followers=X['statuses_retweeted_status_user_followers_count'].mean()
        followers_List.append(user_followers)
 
        retweet_count=X['statuses_retweet_count'].mean()
        retweetcount_List.append(retweet_count)

        og_listed_count=X['statuses_retweeted_status_user_listed_count'].mean()
        listedcount_List.append(og_listed_count)

        source=X.statuses_source.iloc[0]
        source_List.append(source)

        data_tuples = list(zip(text_List,input_query,hours_List,ave_tweets_perday,location_List,retweetcount_List,followers_List,listedcount_List,source_List,Length_List,iso_code,userdescription,user_Created,fav_Count,verif,user_Statuses,has_Image,Days_Active))

        MicroblogText_df=pd.DataFrame(data_tuples, columns=['Status Text','input_query','Hours retweeted','ave_tweets_perday','Location of user','number of times retweeted','number of followers','number of times listed','source','Length','iso_code','user_description','userCreated','favCount','verified','userStatuses','hasImage','DaysActive'])

        return MicroblogText_df

import pickle


import gensim
import gensim.corpora as corpora

from gensim.corpora import Dictionary
from gensim.models import ldamodel
from gensim.utils import simple_preprocess 
from gensim.models import CoherenceModel

pickle_in = open("Topic_classfier.pkl", "rb") 
Topic_clf=pickle.load(pickle_in)

# =============================================================================
# def TP_prediction(doc,model=Topic_clf):
#     
#     new_text_corpus =  model.id2word.doc2bow(doc.split())
#     
#     
#     return model(new_text_corpus)
# =============================================================================
    

def main():
    
    # Title
    st.title("Covid19za Consortium")
    st.subheader("Analysis and Predictor Models for Covid19 Microblog data ")
    st.write("This app uses a microblog, twitter data to help identify communication straegy for health and government officials during pandemics in social platforms We use user and microblog content information to predict if a model will Trend  and rate of tranmission of a blog through statistcal distribution approaches, we also further cluster the text between SA and NonSA and determine sentiment betweeen the groups.")
    st.subheader("Creating Sub-table")
    
    #Creating side bar to upload the file
    st.sidebar.subheader("Model and Visualization Headings")
    menu=["","Bulk prediction","Single prediction"]
    choice=st.sidebar.selectbox("Choose data load method", menu)
    
    if choice== "Bulk prediction":
        Data_file=st.sidebar.file_uploader(label="Upload csv raw file", type=['xlsx'])
        
        if st.button('Predict'):
          data=pd.read_excel(Data_file)
         
          data= load_new_data(data)
         
          data=sub_df(data)

          st.dataframe(data)
          
    if choice=="Single prediction":
        st.sidebar.subheader("User Information:")
        number_of_followers=st.sidebar.number_input("number of followers",min_value=0, max_value=10000000,step=1)
        number_of_times_listed=st.sidebar.number_input("number of times listed",step=1)
        fav_Count=st.sidebar.number_input("fav Count", step=1)
        has_image=st.sidebar.number_input("has image (1-yes 0-No)",min_value=0,max_value=1, step=1)
        has_decription=st.sidebar.number_input("has decription (1-yes 0-No)",min_value=0,max_value=1, step=1)
        Date_user_created=st.sidebar.date_input("Date user created")
        Microblog_text=st.sidebar.text_input("Microblog text")
        
        features={'number_of_followers': number_of_followers,
            'number_of_times_listed': number_of_times_listed, 'fav_Count': fav_Count,
            'has image': has_image, 'has_decription': has_decription,
            'Date_user_created': Date_user_created, 'Microblog_text': Microblog_text
            }
        
        data = pd.DataFrame([features])
        
        if st.button('Predict'):
            
        
            st.table(data)  
        
        
# =============================================================================
#      prediction = predict_quality(model, features_df)
#      st.write(' Based on feature values, your wine quality is '+ str(prediction))
#     
#     if Data_file is not None:
# 
#         df=pd.read_excel(Data_file)
#         data=load_new_data(df)
#    
#         st.dataframe(df)
#         st.dataframe(data)
#         
#         subdf=sub_df(data=df)
#         st.dataframe(subdf)
# =============================================================================
  
    
    
if __name__=='__main__':
    main()
    
    
    


# =============================================================================
# #nltk.download('stopwords')
# #nltk.download('wordnet')
# 
# from google.colab import drive
# drive.mount('/content/drive')
# 
# #data = pd.read_excel("/content/drive/Shareddrives/Capstone Project/Copy of Copy of 23_03_2021_MITDataset_Group3.xlsx")
# 
# data = pd.read_excel("/content/drive/Shareddrives/Capstone Project/EDA_sample4.xlsx")
# 
# labels = pd.read_excel("/content/drive/Shareddrives/Capstone Project/23_03_2021_Library (1).xlsx")
# 
# Label_list=labels["field_name"].values.tolist()
# 
# data.columns=Label_list
# 
# data['statuses_created_at'] =  pd.to_datetime(data['statuses_created_at'])
# 
# data['statuses_created_at'] =  pd.to_datetime(data['statuses_created_at'],format="%Y-%m-%d")
# 
# data['statuses_retweeted_status_created_at'] =  pd.to_datetime(data['statuses_retweeted_status_created_at'])
# 
# data['statuses_retweeted_status_created_at'] =  pd.to_datetime(data['statuses_retweeted_status_created_at'],format="%Y-%m-%d")
# 
# data['statuses_retweeted_status_user_created_at'] =  pd.to_datetime(data['statuses_retweeted_status_user_created_at'])
# 
# data['statuses_retweeted_status_user_created_at'] =  pd.to_datetime(data['statuses_retweeted_status_user_created_at'],format="%Y-%m-%d")
# 
# today=datetime.now(tz=timezone.utc)
# #d = datetime.datetime.strptime(str(today), '%Y-%m-%d')
# #d=d.strftime('%b %d,%Y')
# data['today']=today
# 
# data['today'] =  pd.to_datetime(data['today'])
# 
# data['today'] =  pd.to_datetime(data['today'],format="%Y-%m-%d")
# 
# data['today']
# =============================================================================

# =============================================================================
# =============================================================================
# # data['statuses_retweeted_status_user_created_at']
# # 
# # data.head()
# # 
# # """### Identifying highest retweeted Microblogs"""
# # 
# # from collections import Counter
# # # status_id -- To determine the tweets with the highest retweets
# # MyList_tweet = data['statuses_retweeted_status_id']
# # k=Counter(MyList_tweet)
# # 
# # D_Tweet_id=pd.DataFrame.from_dict(k, orient='index').reset_index()
# # 
# # D_Tweet_id.columns=["statuses_retweeted_status_id","value"]
# # 
# # D_Tweet_id.sort_values(by=['value'], inplace=True,ascending=False)
# # 
# # D_Tweet_id_top=D_Tweet_id.head(100)
# # D_Tweet_id_bottom=D_Tweet_id[D_Tweet_id['statuses_retweeted_status_id'].notna()].tail(10)
# # 
# # """Visual time series of most retweeted microblog"""
# # 
# # print("The Most retweeted statuses")
# # D_Tweet_id
# # 
# # stts_Id_top=D_Tweet_id["statuses_retweeted_status_id"].to_list()
# # 
# # type(stts_Id_top[0])
# # 
# # dftop_tweet=data.loc[data['statuses_retweeted_status_id'].isin(stts_Id_top)]
# # 
# # dftop_tweet1=data.loc[data['statuses_retweeted_status_id']==(stts_Id_top[0])]
# # 
# # dftop_Microblog1=data.loc[data['statuses_retweeted_status_user_id']==(stts_Id_top[0])]
# # 
# # dftop_tweet1[['statuses_created_at','statuses_entities_user_mentions[0]_name','statuses_retweeted_status_created_at','statuses_retweeted_status_id','statuses_retweeted_status_text','statuses_retweeted_status_user_location','statuses_retweeted_status_user_description','statuses_retweeted_status_user_followers_count','statuses_retweeted_status_user_listed_count','statuses_retweeted_status_user_favourites_count','statuses_retweet_count','statuses_source','statuses_retweeted_status_user_description','today']]
# # 
# # time_series = pd.DataFrame(dftop_tweet1['statuses_created_at'].value_counts().reset_index())
# # time_series.columns = ['date', 'count']
# # 
# # time_series
# # 
# # plt.plot(time_series['date'], time_series['count'], '-')
# # plt.xticks(rotation='vertical')
# # 
# # """### identify days microblog was retweeted"""
# # 
# # print(dftop_tweet1.statuses_created_at.max())
# # 
# # dftop_tweet1['statuses_retweeted_status_created_at'] =  pd.to_datetime(dftop_tweet1['statuses_retweeted_status_created_at'])
# # 
# # print((dftop_tweet1.statuses_created_at.max() - dftop_tweet1.statuses_retweeted_status_created_at.min()).days)
# # 
# # import math
# # stts_Id_top = [x for x in stts_Id_top if math.isnan(x) == False]
# # 
# # (dftop_tweet1.statuses_retweeted_status_created_at.max() - dftop_tweet1.statuses_retweeted_status_user_created_at.min()).days
# # 
# # """### Creating new data set focused on orignal microblog user"""
# # 
# # import datetime
# # 
# # def avg_day(date_col):
# #   time_series = pd.DataFrame(date_col.value_counts().reset_index())
# #   time_series.columns = ['date', 'count']
# #   time_series['tweetdate'] = time_series['date'].map(lambda x: x.date())
# #   grp_date = time_series.groupby('tweetdate')
# #   tweet_by_date = pd.DataFrame(grp_date.size(), columns=['num_'])
# #   tweet_by_date_avg=tweet_by_date.num_.mean()
# # 
# #   return tweet_by_date_avg
# # 
# # hours_List=[]
# # min_List=[]
# # text_List=[]
# # location_List=[]
# # followers_List=[]
# # retweetcount_List=[]
# # listedcount_List=[]
# # source_List=[]
# # Length_List=[]
# # ExclamationMarks=[]
# # input_query=[]
# # iso_code=[]
# # userdescription=[]
# # user_Created=[]
# # Days_Active=[]
# # fav_Count=[]
# # verif=[]
# # user_Statuses=[]
# # has_Image=[]
# # ave_tweets_perday=[]
# # id=[]
# # 
# # stts_Id_top
# # 
# # for i in stts_Id_top:
# #     X=data.loc[data['statuses_retweeted_status_id']==i]
# #     original_date= X.statuses_retweeted_status_created_at.min()
# #     max_date=X.statuses_created_at.max()
# #     diff=(X.statuses_created_at.max() - X.statuses_retweeted_status_created_at.min())
# #     days=diff.days
# #     seconds=diff.seconds
# #     hours=(days * 24) + (seconds // 3600)
# #     minutes = (seconds % 3600) // 60
# #     hours_List.append(hours)
# #     min_List.append(minutes)
# # 
# #     id_=i
# #     id.append(id_)
# # 
# #     text= X.statuses_retweeted_status_text.iloc[0]
# #     text_List.append(text)
# # 
# #     date_col=X['statuses_created_at']
# #     ave_day=avg_day(date_col)
# #     ave_tweets_perday.append(ave_day)
# #     
# #     Length=len(str(text))
# #     Length_List.append(Length)
# #     
# #     hashtag=X.input_query.iloc[0]
# #     input_query.append(hashtag)
# #     
# #     iso=X.statuses_metadata_iso_language_code.iloc[0]
# #     iso_code.append(iso)
# #     
# #     user_description=X.statuses_retweeted_status_user_description.iloc[0]
# #     userdescription.append(user_description)
# # 
# #     userCreated=X.statuses_retweeted_status_user_created_at.iloc[0]
# #     user_Created.append(userCreated)
# # 
# #     diff_Created=(X.statuses_retweeted_status_created_at.max() - X.statuses_retweeted_status_user_created_at.min())
# #     DaysActive=diff_Created.days
# #     Days_Active.append(DaysActive)
# # 
# #     favCount=X.statuses_retweeted_status_user_favourites_count.iloc[0]
# #     fav_Count.append(favCount)
# # 
# #     userStatuses=X.statuses_retweeted_status_user_statuses_count.iloc[0]
# #     user_Statuses.append(userStatuses)
# # 
# #     verified=X.statuses_retweeted_status_user_verified.iloc[0]
# #     verif.append(verified)
# #   
# #     hasImage=X.statuses_retweeted_status_user_profile_use_background_image.iloc[0]
# #     has_Image.append(hasImage)
# # 
# #     location=X.statuses_retweeted_status_user_location.iloc[0]
# #     location_List.append(location)
# # 
# #     user_followers=X['statuses_retweeted_status_user_followers_count'].mean()
# #     followers_List.append(user_followers)
# # 
# #     retweet_count=X['statuses_retweet_count'].mean()
# #     retweetcount_List.append(retweet_count)
# # 
# #     og_listed_count=X['statuses_retweeted_status_user_listed_count'].mean()
# #     listedcount_List.append(og_listed_count)
# # 
# #     source=X.statuses_source.iloc[0]
# #     source_List.append(source)
# # 
# # data_tuples = list(zip(text_List,input_query,hours_List,ave_tweets_perday,location_List,retweetcount_List,followers_List,listedcount_List,source_List,Length_List,iso_code,userdescription,user_Created,fav_Count,verif,user_Statuses,has_Image,Days_Active))
# # 
# # MicroblogText_df=pd.DataFrame(data_tuples, columns=['Status Text','input_query','Hours retweeted','ave_tweets_perday','Location of user','number of times retweeted','number of followers','number of times listed','source','Length','iso_code','user_description','userCreated','favCount','verified','userStatuses','hasImage','DaysActive'])
# # 
# # MicroblogText_df
# # 
# # """### **4. Text Analysis: Language translation of newData**"""
# # 
# # !pip install deep-translator
# # !pip install langdetect
# # 
# # from langdetect import detect
# # from deep_translator import GoogleTranslator
# # 
# # #Create dataframe of the undetected language
# # #data_indet=MicroblogText_df[MicroblogText_df['iso_code']=='und']
# # 
# # #count of the undetected languages
# # #print("Count of undetected languages from main dataset: "+str(len(data_indet)))
# # 
# # #create dataframe of non-english languages[including undetected]
# # #data_non_eng=MicroblogText_df[MicroblogText_df['iso_code']!='en']
# # 
# # #data_non_eng
# # 
# # """#### **Detect language**"""
# # 
# # lang_code=[] # to store detected language iso code
# # for i in MicroblogText_df['Status Text']:
# # 
# #   try:
# #     lang = detect(i) # performing language detection
# #   except:
# #     lang="und" # put error if language is undetected/or cell is empty[any case]
# #     
# #   lang_code.append(lang)
# # 
# # MicroblogText_df['LangDet_code']=lang_code #Create a column of the newly detected language iso code
# # 
# # len(MicroblogText_df['LangDet_code'])
# # 
# # """#### **Compare detected iso language code and iso language code on the data**"""
# # 
# # #Check if the detected iso code are the same as the iso code on the data
# # MicroblogText_df['Iso_Code_Comparison'] = np.where((MicroblogText_df['LangDet_code'] == MicroblogText_df['iso_code']), 'Yes', 'No')
# # 
# # len(MicroblogText_df['Iso_Code_Comparison'])
# # 
# # pd.DataFrame(MicroblogText_df['LangDet_code'].value_counts()).sum()
# # 
# # MicroblogText_df[MicroblogText_df['LangDet_code']=='und']
# # 
# # """##### **Exclude the un-detected language[the one throwing errors-, eg emails, url] and detected as English on the dataframe**"""
# # 
# # data_eng=MicroblogText_df[MicroblogText_df['LangDet_code'] =='en']
# # data_un=MicroblogText_df[MicroblogText_df['LangDet_code'] =='und']
# # data_translated=MicroblogText_df[MicroblogText_df['LangDet_code'] !='en']
# # data_translated=data_translated[data_translated['LangDet_code'] !='und']
# # 
# # data_not_translate=data_eng.append(data_un,ignore_index = True)
# # 
# # print("The size of the dataframe: " + str(len(MicroblogText_df)))
# # print("The count of un-detected texts/rows: " + str(len(data_un[data_un['LangDet_code']=='und'])))
# # print('The count of english text: '+ str(len(data_eng[data_eng['LangDet_code']=='en'])))
# # print('The count of non english: '+ str(len(data_translated[data_translated['LangDet_code']!='en'])))
# # 
# # data_translated=data_translated.reset_index(drop=True) #reset index
# # 
# # data_translated
# # 
# # """**Extract out texts that are detected as english**"""
# # 
# # #data_translate_en=data_non_eng[data_non_eng['LangDet_code']=='en'] # Creat a dataframe of English detected languages [ to be merged with the other data after translation]
# # 
# # #print("The count of the texts that are detected as english: "+str(len(data_translate_en))) #Get a count of languages detected as English
# # 
# # #data_translateddata_translated.append(data_translate_en,ignore_index = True)
# # 
# # """#### **Transalate non-English text**"""
# # 
# # data_translated['LangDet_code'].value_counts()
# # 
# # translated_text=[] #To store translated text
# # for i in data_translated['Status Text'].index:
# #     to_translate = data_translated['Status Text'].iloc[i]
# #     translated = GoogleTranslator(source=data_translated['LangDet_code'].iloc[i], target='en').translate(to_translate) #Perform translation
# #     translated_text.append(translated)
# # 
# # data_translated['Translated_tweet']=translated_text # Translated text Column
# # 
# # data_translated['Status Text']=data_translated['Translated_tweet']
# # 
# # del data_translated['Translated_tweet'] # delete additional column[since it has overriden 'statuses_text']
# # 
# # data_translated.shape
# # 
# # Final_Dataset=data_not_translate.append(data_translated,ignore_index = True) # Join Originally English identified, Translated datasets and English detected Datasets
# # 
# # Final_Dataset
# # 
# # 
# # 
# # """### Genism Model unsupervised training"""
# # 
# # from sklearn.feature_extraction.text import TfidfVectorizer
# # from sklearn.cluster import KMeans
# # import numpy as np
# # import pandas as pd
# # 
# # import nltk; nltk.download('stopwords')
# # 
# # import spacy.cli
# # spacy.cli.download("en_core_web_sm")
# # 
# # pip install gensim
# # 
# # pip install pyLDAvis
# # 
# # import pyLDAvis
# # 
# # # Commented out IPython magic to ensure Python compatibility.
# # import re
# # from pprint import pprint
# # import gensim
# # import gensim.corpora as corpora
# # from gensim.utils import simple_preprocess
# # from gensim.models import CoherenceModel
# # 
# # import spacy
# # 
# # import pyLDAvis
# # #import pyLDAvis.gensim
# # import matplotlib.pyplot as plt
# # # %matplotlib inline
# # 
# # import logging
# # logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)
# # 
# # import warnings
# # warnings.filterwarnings("ignore", category=DeprecationWarning)
# # 
# # import pyLDAvis.gensim_models
# # 
# # from nltk.corpus import stopwords
# # stop_words=stopwords.words('english')
# # stop_words.extend(['from','subject','re','edu','use','https','la','del','Get','check','school','continue','country','family','worker','people','break','question','call','activity','announce'])
# # 
# # #Function which cleans up document/corpus
# # def Corpus(Document):
# # 
# #   Document=Document.apply(str)
# #   Document=Document.str.replace("[^a-zA-Z#]"," ")
# #   Document=Document.apply(lambda x: " ".join([w for w in x.split() if len(w)>3]))
# # 
# #   return Document
# # 
# # Doc_Final_df=Corpus(Document=Final_Dataset["Status Text"])
# # Doc_Full_df=Corpus(Document=data["statuses_text"])
# # 
# # Doc_Final_df
# # 
# # Doc_Full_df
# # 
# # #function to split corpus to words
# # def sent_to_words(sentences):
# #   for sentence in sentences:
# #     yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))
# # 
# # data_words=list(sent_to_words(Doc_Final_df))
# # data_words_fulldf=list(sent_to_words(Doc_Full_df))
# # 
# # data_words_fulldf
# # 
# # def remove_stopwords(texts):
# #     return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]
# # 
# # def make_bigrams(texts):
# # 
# #   bigram=gensim.models.Phrases(texts,min_count=5,threshold=100)
# #   bigram_mod=gensim.models.phrases.Phraser(bigram)
# #   return [bigram_mod[doc] for doc in texts]
# # 
# # def make_trigrams(texts):
# #   bigram=gensim.models.Phrases(texts,min_count=5,threshold=100)
# #   bigram_mod=gensim.models.phrases.Phraser(bigram)
# #   trigram=gensim.models.Phrases(bigram[texts],threshold=100)
# #   trigram_mod=gensim.models.phrases.Phraser(trigram)
# #   return [trigram_mod[bigram_mod[doc]] for doc in texts]
# # 
# # def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
# #     texts_out = []
# #     for sent in texts:
# #         doc = nlp(" ".join(sent)) 
# #         texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
# #     return texts_out
# # 
# # def lemmatize(texts):
# #   # Remove Stop Words
# #   data_words_nostops = remove_stopwords(data_words)
# #   # Form Bigrams
# #   data_words_bigrams = make_bigrams(data_words_nostops)
# #   # Initialize spacy 'en' model, keeping only tagger component (for efficiency)
# #   nlp = spacy.load('en', disable=['parser', 'ner'])
# #   # Do lemmatization keeping only noun, adj, vb, adv
# #   data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
# # 
# #   return data_lemmatized
# # 
# # # Remove Stop Words
# # data_words_nostops = remove_stopwords(data_words)
# #   # Form Bigrams
# # data_words_bigrams = make_bigrams(data_words_nostops)
# #   # Initialize spacy 'en' model, keeping only tagger component (for efficiency)
# # nlp = spacy.load('en', disable=['parser', 'ner'])
# #   # Do lemmatization keeping only noun, adj, vb, adv
# # data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
# # 
# # data_lemmatized = lemmatize(data_words)
# # data_lemmatized_fullfdf=lemmatize(data_words_fulldf)
# # 
# # print(data_lemmatized[:1])
# # 
# # print(data_lemmatized_fullfdf[:1])
# # 
# # #function to create dictionary and term document Frequecy
# # def corpus_(lemma_data):
# #   # Create Dictionary
# #   id2word = corpora.Dictionary(lemma_data)
# #   # Create Corpus
# #   texts_df = lemma_data
# #   # Term Document Frequency
# #   corpus = [id2word.doc2bow(text) for text in texts_df]
# # 
# #   return corpus, id2word
# # 
# # corpus,id2word =corpus_(data_lemmatized)
# # #corpus_df, id2word_df=corpus_(data_fulldf_lemmatized)
# # 
# # print(corpus[:1])
# # 
# # [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]
# # 
# # def lda_model_(corpus, id2word):
# #   lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
# #                                            id2word=id2word,
# #                                            num_topics=3, 
# #                                            random_state=100,
# #                                            update_every=1,
# #                                            chunksize=100,
# #                                            passes=10,
# #                                            alpha='auto',
# #                                            per_word_topics=True)
# #   return lda_model
# # 
# # lda_model=lda_model_(corpus,id2word)
# # 
# # pprint(lda_model.print_topics())
# # doc_lda = lda_model[corpus]
# # 
# # topics = [[(term, round(wt, 3)) for term, wt in lda_model.show_topic(n, topn=20)] for n in range(0, lda_model.num_topics)]
# # 
# # # Compute Perplexity
# # print('\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.
# # 
# # # Compute Coherence Score
# # coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')
# # coherence_lda = coherence_model_lda.get_coherence()
# # print('\nCoherence Score: ', coherence_lda)
# # 
# # pyLDAvis.enable_notebook()
# # vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)
# # vis
# # 
# # !wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip
# # 
# # !unzip mallet-2.0.8.zip
# # 
# # mallet_path = '/content/mallet-2.0.8/bin/mallet'
# # 
# # ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=2, id2word=id2word)
# # 
# # # Show Topics
# # pprint(ldamallet.show_topics(formatted=False))
# # 
# # # Compute Coherence Score
# # coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')
# # coherence_ldamallet = coherence_model_ldamallet.get_coherence()
# # print('\nCoherence Score: ', coherence_ldamallet)
# # 
# # def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=2):
# #     """
# #     Compute c_v coherence for various number of topics
# # 
# #     Parameters:
# #     ----------
# #     dictionary : Gensim dictionary
# #     corpus : Gensim corpus
# #     texts : List of input texts
# #     limit : Max num of topics
# # 
# #     Returns:
# #     -------
# #     model_list : List of LDA topic models
# #     coherence_values : Coherence values corresponding to the LDA model with respective number of topics
# #     """
# #     coherence_values = []
# #     model_list = []
# #     for num_topics in range(start, limit, step):
# #         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)
# #         model_list.append(model)
# #         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
# #         coherence_values.append(coherencemodel.get_coherence())
# # 
# #     return model_list, coherence_values
# # 
# # model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=2)
# # 
# # limit=40; start=2; step=2;
# # x = range(start, limit, step)
# # plt.plot(x, coherence_values)
# # plt.xlabel("Num Topics")
# # plt.ylabel("Coherence score")
# # plt.legend(("coherence_values"), loc='best')
# # plt.show()
# # 
# # for m, cv in zip(x, coherence_values):
# #     print("Num Topics =", m, " has Coherence Value of", round(cv, 4))
# # 
# # optimal_model = model_list[12]
# # model_topics = optimal_model.show_topics(formatted=False)
# # pprint(optimal_model.print_topics(num_words=10))
# # 
# # def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):
# #     # Init output
# #     sent_topics_df = pd.DataFrame()
# # 
# #     # Get main topic in each document
# #     for i, row in enumerate(ldamodel[corpus]):
# #         row = sorted(row, key=lambda x: (x[1]), reverse=True)
# #         # Get the Dominant topic, Perc Contribution and Keywords for each document
# #         for j, (topic_num, prop_topic) in enumerate(row):
# #             if j == 0:  # => dominant topic
# #                 wp = ldamodel.show_topic(topic_num)
# #                 topic_keywords = ", ".join([word for word, prop in wp])
# #                 sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
# #             else:
# #                 break
# #     sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']
# # 
# #     # Add original text to the end of the output
# #     contents = pd.Series(texts)
# #     sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
# #     return(sent_topics_df)
# # 
# # df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data_lemmatized)
# # 
# # # Format
# # df_dominant_topic = df_topic_sents_keywords.reset_index()
# # df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']
# # 
# # # Show
# # df_dominant_topic.head(10)
# # 
# # """### Average sentiment for each microblog"""
# # 
# # pip install vaderSentiment
# # 
# # import vaderSentiment
# # 
# # from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer 
# # import pandas as pd
# # 
# # sentences = text_List
# # sentiment = list()
# # for sentence in sentences:
# #     sent_vader = list(SentimentIntensityAnalyzer().polarity_scores(sentence).values())
# #     sentiment.append(sent_vader[3])
# # 
# # sentiment_sentences = pd.DataFrame({'sentence':sentences,'sentiment':sentiment})
# # 
# # sentiment_sentences
# # 
# # pip install pycorenlp
# # 
# # import pycorenlp
# # 
# # import subprocess
# # 
# # #subprocess.Popen(['java','-mx4g','-cp','*','edu.stanford.nlp.pipeline.StanfordCoreNLPServer'],
# # #cwd= "C:\stanford-corenlp-full-2018-02-27", shell=True, stdout= subprocess.DEVNULL,
# # #stderr=subprocess.STDOUT)
# # 
# # #from pycorenlp import StanfordCoreNLP
# # #import pandas as pd
# # 
# # #sentences = text_List
# # #sentiment = list()
# # 
# # #for sentence in sentences:
# #  #   nlp = StanfordCoreNLP('http://semanticparsing:9000')
# #   #  sentiment_stanford = nlp.annotate(sentence, properties={'timeout': '500000','annotators': 'sentiment', 'outputFormat': 'json'})
# #    # sentiment_stanford = sentiment_stanford['sentences'][0]['sentimentValue']
# #     #sentiment.append(sentiment_stanford)
# # 
# # #sentiment_sentences = pd.DataFrame({'sentence':sentences,'sentiment':sentiment})
# # #labels = {"0": "very negative", "1": "negative", "2":"neutral", "3":"positive", "4":"very positive"}
# # #sentiment_sentences['sentiment'] = sentiment_sentences.sentiment.apply(lambda x: labels[x])
# # 
# # #sentiment_sentences.to_csv("stanfordnlp.csv", sep = ";", encoding="utf-8",quotechar="'",index=False)
# # 
# # """### Number of urls text has"""
# # 
# # import re
# #   
# # def Find(string):
# #   
# #     # findall() has been used 
# #     # with valid conditions for urls in string
# #     regex = r"(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:'\".,<>?«»“”‘’]))"
# #     url = re.findall(regex,string)      
# #     return len(url)
# #       
# # # Driver Code
# # string = 'My Profile: https://auth.geeksforgeeks.org/user/Chinmoy%20Lenka/articles in the portal of http://www.geeksforgeeks.org/'
# # print("Urls: ", Find(string))
# # 
# # sentences = text_List
# # url = list()
# # for sentence in sentences:
# #     hasUrl = Find(sentence)
# #     url.append(hasUrl)
# # 
# # nourls = pd.DataFrame({'sentence':sentences,'urls':url})
# # 
# # nourls
# # 
# # """### User and content based feature table"""
# # 
# # Final_Dataset['Sentiment']=sentiment_sentences["sentiment"]
# # Final_Dataset['No_Urls']=nourls["urls"]
# # Final_Dataset['Topic']=df_dominant_topic['Dominant_Topic']
# # Final_Dataset['avg_tweets_day']=MicroblogText_df['ave_tweets_perday']
# # 
# # Final_Dataset
# # 
# # Final_Dataset['user_description']=Final_Dataset['user_description'].astype(str)
# # 
# # Final_Dataset['verified']=Final_Dataset['verified'].astype(str)
# # 
# # Final_Dataset['hasImage']=Final_Dataset['hasImage'].astype(str)
# # 
# # Final_Dataset['No_Urls']=Final_Dataset['No_Urls'].astype(str)
# # 
# # Final_Dataset['Topic']=Final_Dataset['Topic'].astype(str)
# # 
# # def C(row):
# #   if(len(row['user_description'])>1):
# #     val="1"
# # 
# #   else:
# #     val="0"
# #         
# #   return val
# # 
# # Final_Dataset['has_description']=Final_Dataset.apply(C,axis=1)
# # 
# # Final_Dataset.describe()
# # 
# # def A(row):
# #   if(row['number of times retweeted']>4000):
# #     val="1"
# # 
# #   else:
# #     val="0"
# #         
# #   return val
# # 
# # Final_Dataset['TrendingStatus']=Final_Dataset.apply(A,axis=1)
# # 
# # Final_Dataset['TrendingStatus'].value_counts()
# # 
# # def B(row):
# #   if(row['Sentiment']>0):
# #     val="Pos"
# #   elif(row['Sentiment']<0):
# #     val="Neg"
# # 
# #   else:
# #     val="neu"
# #         
# #   return val
# # 
# # Final_Dataset['Sentiment']=Final_Dataset.apply(B,axis=1)
# # 
# # Final_Dataset['Sentiment'].value_counts()
# # 
# # grp=Final_Dataset[Final_Dataset['TrendingStatus']=='1']
# # 
# # grp
# # 
# # grp.groupby("Sentiment").apply(lambda s: pd.Series({ 
# #     "TrendingStatus": s["TrendingStatus"].count()
# # }))
# # 
# # len(ave_tweets_perday)
# # 
# # Final_Dataset
# # 
# # """### Trending Model based on user, content and diffusion data"""
# # 
# # y=Final_Dataset["TrendingStatus"]
# # 
# # Final_Dataset.columns
# # 
# # X=pd.DataFrame(Final_Dataset[["number of followers","number of times listed",
# #                               "Length","favCount", "verified","userStatuses", "hasImage", "DaysActive","Sentiment","No_Urls","Topic", "has_description"]])
# # 
# # from sklearn.preprocessing import LabelEncoder
# # 
# # labelencoder = LabelEncoder()
# # 
# # X['Sentiment'] = labelencoder.fit_transform(X['Sentiment'])
# # 
# # X
# # 
# # y.value_counts()
# # 
# # """imbalanced classes can cause a biase in the classification so we use the technique of random oversampling of class 1 to obtain balanced classes using imblean module:"""
# =============================================================================
# =============================================================================

# =============================================================================
# import imblearn
# 
# from imblearn.over_sampling import SMOTE
# 
# from imblearn.over_sampling import RandomOverSampler
# 
# ros = RandomOverSampler(random_state=42)
# 
# variablex_ros, y_ros=ros.fit_resample(X, y)
# 
# y_ros
# 
# smote = SMOTE()
# 
# # fit predictor and target variable
# x_smote, y_smote = smote.fit_resample(X, y)
# 
# print('Original dataset shape', Counter(y))
# print('Resample dataset shape', Counter(y_ros))
# 
# y_bal=pd.DataFrame(y_ros)
# 
# y_bal.value_counts()
# 
# X.columns
# 
# X_bal=pd.DataFrame(variablex_ros,columns=['number of followers', 'number of times listed', 'Length', 'favCount',
#        'verified', 'userStatuses', 'hasImage', 'DaysActive', 'Sentiment',
#        'No_Urls', 'Topic', 'has_description'])
# 
# from sklearn import preprocessing
# 
# from sklearn.model_selection import train_test_split
# from sklearn import svm
# 
# min_max_scaler = preprocessing.MinMaxScaler()
# X_bal= min_max_scaler.fit_transform(X_bal)
# X_bal= pd.DataFrame(X_bal,columns=['number of followers', 'number of times listed', 'Length', 'favCount',
#        'verified', 'userStatuses', 'hasImage', 'DaysActive', 'Sentiment',
#        'No_Urls', 'Topic', 'has_description'])
# 
# X_train, X_test, y_train, y_test = train_test_split(X_bal, y_bal, test_size=0.3,random_state=109) # 70% training and 30% test
# 
# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10,random_state=7) # 10% validation
# 
# X_train.info()
# 
# from sklearn.model_selection import GridSearchCV
# 
# param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'linear', 'sigmoid']}
# 
# y_train=y_train.values.ravel()
# 
# grid = GridSearchCV(svm.SVC(),param_grid,refit=True,verbose=2)
# grid.fit(X_train,y_train)
# 
# print(grid.best_estimator_)
# 
# from sklearn.metrics import confusion_matrix
# from sklearn.metrics import classification_report
# 
# 
# grid_predictions = grid.predict(X_test)
# print(confusion_matrix(y_test,grid_predictions))
# print(classification_report(y_test,grid_predictions))
# 
# #Create a svm Classifier
# clf = svm.SVC(C=0.1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
#     decision_function_shape='ovr', degree=3, gamma=1, kernel='linear',
#     max_iter=-1, probability=False, random_state=None, shrinking=True,
#     tol=0.001, verbose=False) # Linear Kernel
# 
# #Train the model using the training sets
# clf.fit(X_train, y_train)
# 
# #Predict the response for test dataset
# y_pred = clf.predict(X_test)
# 
# #Import scikit-learn metrics module for accuracy calculation
# from sklearn import metrics
# 
# # Model Accuracy: how often is the classifier correct?
# print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
# 
# y_val_pred = clf.predict(X_val)
# 
# print("Accuracy:",metrics.accuracy_score(y_val, y_val_pred))
# 
# def f_importances(coef, names, top=-1):
#     imp = coef
#     imp, names = zip(*sorted(list(zip(imp, names))))
# 
#     # Show all features
#     if top == -1:
#         top = len(names)
# 
#     plt.barh(range(top), imp[::-1][0:top], align='center')
#     plt.yticks(range(top), names[::-1][0:top])
#     plt.show()
# 
# f_importances(abs(clf.coef_[0]), names=X_train.columns, top=10)
# 
# """Adding noise to determine stability: randomly flipping labels"""
# 
# import random
# 
# labels=y_train
# n = len(labels)
# percent = 0.2
# 
# #labels_dt=y_train_dt
# #n_dt = len(labels_dt)
# #percent = 0.2
# 
# for i in range(n):
#   if i in random.sample(list(range(n)),int(percent*n)):
#     if labels[i]=='0':
#       labels[i]='1'
#     else:
#       labels[i]='0'
# 
# pd.DataFrame(labels).value_counts()
# 
# grid_stabili = GridSearchCV(svm.SVC(),param_grid,refit=True,verbose=2)
# grid_stabili.fit(X_train,labels)
# 
# print(grid_stabili.best_estimator_)
# 
# from sklearn.metrics import confusion_matrix
# from sklearn.metrics import classification_report
# 
# 
# grid_predictions_stabl = grid_stabili.predict(X_test)
# print(confusion_matrix(y_test,grid_predictions_stabl))
# print(classification_report(y_test,grid_predictions_stabl))
# 
# """RandomForest"""
# 
# X_train_dt, X_test_dt, y_train_dt, y_test_dt = train_test_split(X, y, test_size=0.3,random_state=109) # 70% training and 30% test
# 
# X_train_dt, X_val_dt, y_train_dt, y_val_dt = train_test_split(X_train_dt, y_train_dt, test_size=0.10,random_state=7) # 10% validation
# 
# from sklearn.model_selection import train_test_split
# from sklearn import model_selection
# from sklearn.ensemble import RandomForestClassifier
# 
# rfc = RandomForestClassifier()
# rfc.fit(X_train_dt,y_train_dt)
# # predictions
# rfc_predict = rfc.predict(X_test_dt)
# 
# from sklearn.model_selection import cross_val_score
# from sklearn.metrics import classification_report, confusion_matrix
# 
# rfc_cv_score = cross_val_score(rfc, X, y, cv=10, scoring='roc_auc')
# 
# print("=== Confusion Matrix ===")
# print(confusion_matrix(y_test_dt, rfc_predict))
# print('\n')
# print("=== Classification Report ===")
# print(classification_report(y_test_dt, rfc_predict))
# print('\n')
# print("=== All AUC Scores ===")
# print(rfc_cv_score)
# print('\n')
# print("=== Mean AUC Score ===")
# print("Mean AUC Score - Random Forest: ", rfc_cv_score.mean())
# 
# from sklearn.model_selection import RandomizedSearchCV
# # number of trees in random forest
# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]
# # number of features at every split
# max_features = ['auto', 'sqrt']
# 
# # max depth
# max_depth = [int(x) for x in np.linspace(100, 500, num = 11)]
# max_depth.append(None)
# # create random grid
# random_grid = {
#  'n_estimators': n_estimators,
#  'max_features': max_features,
#  'max_depth': max_depth
#  }
# # Random search of parameters
# rfc_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
# # Fit the model
# rfc_random.fit(X_train_dt, y_train_dt)
# # print results
# print(rfc_random.best_params_)
# 
# rfc = RandomForestClassifier(n_estimators=911, max_depth= 100, max_features='sqrt')
# rfc.fit(X_train_dt,y_train_dt)
# rfc_predict = rfc.predict(X_test_dt)
# rfc_cv_score = cross_val_score(rfc, X, y, cv=10, scoring='roc_auc')
# print("=== Confusion Matrix ===")
# print(confusion_matrix(y_test_dt, rfc_predict))
# print('\n')
# print("=== Classification Report ===")
# print(classification_report(y_test_dt, rfc_predict))
# print('\n')
# print("=== All AUC Scores ===")
# print(rfc_cv_score)
# print('\n')
# print("=== Mean AUC Score ===")
# print("Mean AUC Score - Random Forest: ", rfc_cv_score.mean())
# 
# """validation"""
# 
# rfc_predict_val = rfc.predict(X_val_dt)
# rfc_cv_score_val = cross_val_score(rfc, X, y, cv=10, scoring='roc_auc')
# print("=== Confusion Matrix ===")
# print(confusion_matrix(y_val_dt, rfc_predict_val))
# print('\n')
# print("=== Classification Report ===")
# print(classification_report(y_val_dt, rfc_predict_val))
# print('\n')
# print("=== All AUC Scores ===")
# print(rfc_cv_score_val)
# print('\n')
# print("=== Mean AUC Score ===")
# print("Mean AUC Score - Random Forest: ", rfc_cv_score_val.mean())
# 
# """stability"""
# 
# rfc = RandomForestClassifier(n_estimators=911, max_depth= 100, max_features='sqrt')
# rfc.fit(X_train,labels)
# rfc_predict = rfc.predict(X_test)
# rfc_cv_score = cross_val_score(rfc, X, y, cv=10, scoring='roc_auc')
# print("=== Confusion Matrix ===")
# print(confusion_matrix(y_test, rfc_predict))
# print('\n')
# print("=== Classification Report ===")
# print(classification_report(y_test, rfc_predict))
# print('\n')
# print("=== All AUC Scores ===")
# print(rfc_cv_score)
# print('\n')
# print("=== Mean AUC Score ===")
# print("Mean AUC Score - Random Forest: ", rfc_cv_score.mean())
# 
# """Base model: decsion tree"""
# 
# from sklearn.tree import DecisionTreeClassifier 
# from sklearn.tree import export_graphviz
# from sklearn.externals.six import StringIO  
# from IPython.display import Image  
# import pydotplus
# 
# clf_dt = DecisionTreeClassifier(criterion="entropy", max_depth=3)
# 
# # Train Decision Tree Classifer
# clf_dt = clf_dt.fit(X_train,y_train)
# 
# #Predict the response for test dataset
# y_pred_tree = clf_dt.predict(X_test)
# 
# # Model Accuracy, how often is the classifier correct?
# print("Accuracy:",metrics.accuracy_score(y_test, y_pred_tree))
# 
# print("=== Confusion Matrix ===")
# print(confusion_matrix(y_test, y_pred_tree))
# print('\n')
# print("=== Classification Report ===")
# print(classification_report(y_test, y_pred_tree))
# print('\n')
# 
# from sklearn.externals.six import StringIO  
# from IPython.display import Image  
# from sklearn.tree import export_graphviz
# import pydotplus
# dot_data = StringIO()
# export_graphviz(clf_dt, out_file=dot_data,  
#                 filled=True, rounded=True,
#                 special_characters=True, feature_names = X_train.columns,class_names=['0','1'])
# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
# graph.write_png('diabetes.png')
# Image(graph.create_png())
# 
# """validation"""
# 
# y_pred_treeval = clf_dt.predict(X_val)
# 
# # Model Accuracy, how often is the classifier correct?
# print("Accuracy:",metrics.accuracy_score(y_val, y_pred_treeval))
# 
# print("=== Confusion Matrix ===")
# print(confusion_matrix(y_val, y_pred_treeval))
# print('\n')
# print("=== Classification Report ===")
# print(classification_report(y_val, y_pred_treeval))
# print('\n')
# 
# """##MicroBlog Simirairty Table:
# group by which microblogs are similar to be able to quantify better.
# """
# 
# Final_Dataset.groupby(['Topic']).groups.keys() # identify the number of groups
# 
# X.columns
# 
# X['Sentiment'] = labelencoder.fit_transform(X['Sentiment'])
# 
# X.groupby("Topic").apply(lambda s: pd.Series({ 
#     "number of followers": s["number of followers"].sum(), 
#     "followers sum": s["number of followers"].sum(),
#     "DaysActive": s["DaysActive"].sum(),
#     "Count": s["Topic"].count()
# }))
# 
# y_rT=Final_Dataset['avg_tweets_day']
# 
# X
# 
# X.corr(method='pearson', min_periods=1)
# 
# X.info()
# 
# labelencoder = LabelEncoder()
# 
# X['No_Urls'] = labelencoder.fit_transform(X['No_Urls'])
# X['Topic'] = labelencoder.fit_transform(X['Topic'])
# X['has_description'] = labelencoder.fit_transform(X['has_description'])
# X['verified'] = labelencoder.fit_transform(X['verified'])
# X['hasImage'] = labelencoder.fit_transform(X['hasImage'])
# 
# min_max_scaler = preprocessing.MinMaxScaler()
# X_rt= min_max_scaler.fit_transform(X)
# X_rt= pd.DataFrame(X,columns=['number of followers', 'number of times listed', 'Length', 'favCount',
#        'verified', 'userStatuses', 'hasImage', 'DaysActive', 'Sentiment',
#        'No_Urls', 'Topic', 'has_description'])
# 
# X_train, X_test, y_train, y_test = train_test_split(X_rt, y_rT, test_size=0.3,random_state=109) # 70% training and 30% test
# 
# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10,random_state=7) # 10% validation
# 
# y_rT=pd.DataFrame(y_rT)
# 
# from sklearn.linear_model import LinearRegression
# regressor = LinearRegression()
# regressor.fit(X_train, y_train)#predicting the test set results
# y_pred_n = regressor.predict(X_test)
# 
# import statsmodels.api as sm
# from termcolor import colored as cl
# 
# model = sm.OLS(np.asarray(y_train), np.asarray(X_train)).fit()
# predictions = model.predict(X_train.values)
# model.summary()
# 
# """SVR"""
# 
# param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'linear', 'sigmoid']}
# 
# y_train=y_train.values.ravel()
# 
# pd.plotting.scatter_matrix(X_train.loc[0:,X_train.columns],alpha=0.5,figsize=[25,25],diagonal='hist',s=200,marker='.',edgecolor='black')
# plt.show()
# 
# from sklearn.svm import SVR
# 
# clf_sv = SVR(kernel = 'rbf')
# 
# clf_sv.fit(X_train,y_train)
# 
# clf_sv_pred= clf_sv.predict(X_test)
# 
# clf_sv_val= clf_sv.predict(X_val)
# 
# mean_squared_error(y_test , clf_sv_pred)
# 
# mean_squared_error(y_val , clf_sv_val)
# 
# clf = LinearRegression(n_jobs=-1)
# 
# y_train.values.ravel()
# 
# for k in ['linear','poly','rbf','sigmoid']:
#     clf = svm.SVR(kernel=k)
#     clf.fit(X_train, y_train)
#     confidence = clf.score(X_test, y_test)
#     print(k,confidence)
# 
# import keras.models
# import tensorflow
# from keras.models import Sequential
# from keras.layers import Activation, Dense
# from keras.callbacks import ModelCheckpoint
# 
# NN_model = Sequential()
# 
# # The Input Layer :
# NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))
# 
# # The Hidden Layers :
# NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
# NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
# NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
# 
# # The Output Layer :
# NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))
# 
# # Compile the network :
# NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])
# NN_model.summary()
# 
# checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' 
# checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')
# callbacks_list = [checkpoint]
# 
# NN_model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)
# 
# wights_file = 'Weights-478--18738.19831.hdf5' # choose the best checkpoint 
# #NN_model.load_weights(wights_file) # load it
# NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])
# 
# predictions = NN_model.predict(X_test)
# 
# predictions_val = NN_model.predict(X_val)
# 
# from sklearn.metrics import mean_squared_error
# mean_squared_error(y_test , predictions)
# 
# mean_squared_error(y_val , predictions_val)
# 
# """Random Forest"""
# 
# from sklearn.ensemble import RandomForestRegressor
# from sklearn.metrics import mean_absolute_error as MAE
# 
# from sklearn.metrics import mean_absolute_error as MAE
# 
# model = RandomForestRegressor()
# model.fit(X_train,y_train)
# 
# # Get the mean absolute error on the validation data
# predicted_prices = model.predict(X_test)
# MAE = MAE(y_test , predicted_prices)
# print('Random forest validation MAE = ', MAE)
# 
# from sklearn.metrics import mean_squared_error
# mean_squared_error(y_test , predicted_prices)
# 
# predicted_val = model.predict(X_val)
# 
# mean_squared_error(y_val , predicted_val)
# 
# """XGBoost"""
# 
# import xgboost as xgb 
# from xgboost import XGBRegressor
# 
# XGBModel = XGBRegressor()
# XGBModel.fit(X_train,y_train, verbose=False)
# 
# # Get the mean absolute error on the validation data :
# XGBpredictions = XGBModel.predict(X_test)
# MAE = MAE(y_test , XGBpredictions)
# print('XGBoost validation MAE = ',MAE)
# 
# from sklearn.metrics import r2_score
# r2_score(y_test, XGBpredictions)
# 
# from sklearn.metrics import mean_squared_error
# mean_squared_error(y_test, XGBpredictions)
# 
# XGBpredictions_val = XGBModel.predict(X_val)
# 
# mean_squared_error(y_val, XGBpredictions_val)
# =============================================================================
